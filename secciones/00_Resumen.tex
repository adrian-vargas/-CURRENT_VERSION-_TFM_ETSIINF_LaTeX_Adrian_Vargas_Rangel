\chapter{Resumen}

A medida que los sistemas de inteligencia artificial (IA) se integran en diversos sectores de la sociedad, la necesidad de modelos de aprendizaje automático interpretables se vuelve crucial para su aceptación y uso ético. En áreas críticas como la medicina, donde los resultados de los algoritmos utilizados para diagnósticos deben ser precisos y comprensibles, es esencial desarrollar nuevas técnicas de explicabilidad que faciliten la toma de decisiones y garanticen la fiabilidad de estas tecnologías.

Este Trabajo de Fin de Máster se inspira en la metodología del estudio \emph{“Interpretable Decision Sets: A Joint Framework for Description and Prediction”} \cite{lakkaraju-2016} y desarrolla un cuestionario para evaluar la interpretabilidad de modelos transparentes, como los árboles de decisión y el modelo de Interpretable Decision Sets (IDS) propuesto en dicho estudio. Para la evaluación, se emplean datos sobre el rendimiento académico en matemáticas de estudiantes, proporcionados por Paulo Cortez y disponibles en el \emph{UCI Machine Learning Repository} \cite{cortez-2014}.

El cuestionario está diseñado para extraer las reglas subyacentes de cada modelo, clasificando a los estudiantes como aprobados o no aprobados, y evaluando la capacidad de los usuarios para interpretar correctamente estas reglas, incluso en situaciones ambiguas. Esta evaluación incluye tanto la exactitud de las predicciones como la habilidad de los usuarios para identificar y comprender errores, lo cual es fundamental para fomentar la confianza y el uso efectivo de la IA en decisiones reales.

Con este enfoque, se busca contribuir al campo de la inteligencia artificial explicable proporcionando una base sólida para la construcción de herramientas de investigación que exploren cómo los humanos interpretan las decisiones de modelos transparentes.

El código fuente del cuestionario desarrollado está disponible en el repositorio de GitHub: \href{https://github.com/adrian-vargas/survey-xai}{https://github.com/adrian-vargas/survey-xai}.



%%--------------
\newpage
%%--------------

\chapter{Abstract}

As artificial intelligence (AI) systems become increasingly integrated into various sectors of society, the need for interpretable machine learning models is crucial for their acceptance and ethical use. In critical fields such as medicine, where the results of algorithms used for diagnostics must be both accurate and understandable, it is essential to develop new explainability techniques that facilitate decision-making and ensure the reliability of these technologies.

This Master's Thesis is inspired by the methodology from the study “Interpretable Decision Sets: A Joint Framework for Description and Prediction” \cite{lakkaraju-2016} and develops a questionnaire to assess the interpretability of transparent models, such as decision trees and the Interpretable Decision Sets (IDS) model proposed in that study. For the evaluation, data on students' academic performance in mathematics, provided by Paulo Cortez and available in the UCI Machine Learning Repository \cite{cortez-2014}, were used.

The questionnaire is designed to extract the underlying rules of each model, classifying students as passing or failing, and evaluating the ability of users to correctly interpret these rules, even in ambiguous situations. This evaluation includes both the accuracy of the predictions and the ability of users to identify and understand errors, which is fundamental to fostering trust and effectively using AI in real-world decisions.

This approach aims to contribute to the field of explainable artificial intelligence by providing a solid foundation for building research tools that explore how humans interpret the decisions of transparent models.

The source code for the developed questionnaire is available on the GitHub repository: \href{https://github.com/adrian-vargas/survey-xai}{https://github.com/adrian-vargas/survey-xai}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Final del resumen. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%